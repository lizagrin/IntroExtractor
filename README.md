# Автоматическое выделение таймкодов начала и конца основного содержания эпизодов сериалов
Создатель: Гранкина Елизавета Григорьевна
## Цель проекта

Разработка методов и моделей для автоматического определения точных временных меток начала и конца основного контента эпизодов сериалов. Это позволит онлайн-кинотеатрам автоматически пропускать вступительные заставки и финальные титры, повышая удобство пользователей.

## Задачи проекта

* Сбор и ручная разметка корпуса сериалов.
* Разработка модели детекции границ по видеоряду.
* Разработка модели детекции границ по аудиодорожке.
* Построение мультимодальной архитектуры, объединяющей аудио- и видео-модели.
* Проведение сравнительного анализа эффективности всех разработанных моделей.

## Датасет

Ручная сборка более 100 серий сериалов общей длительностью около 100 часов. Для каждой серии вручную размечены таймкоды после заставки и перед титрами.

Для решения дисбаланса классов применялись:

* Взвешенная функция потерь (больший штраф за ошибки на кадрах титров).
* Балансировка мини-партий данных (примерно 50% кадров титров и 50% основного содержания в каждом пакете).
* Аугментация титров (случайный сдвиг окон ±1 секунда, изменение громкости и небольшие визуальные изменения).

## Архитектура

<img alt="Функциональная схема метода.png" src="img\Функциональная схема метода.png"/>

**Предобработка:**

* Используется ffmpeg для разделения видеопотока на кадры (1 кадр в секунду) и выделения аудиодорожки.

**Видео-модель (ноутбук `notebooks/video/video_detect_pipeline.ipynb`):**

* Используется модель CLIP для получения эмбеддингов кадров.
* Transformer-энкодер (16 слоев) обрабатывает полученные эмбеддинги, учитывая контекст кадров.
* 60 независимых сигмоидных классификаторов определяют вероятность принадлежности каждого кадра к титрам.

**Аудио-модель (ноутбук `notebooks/audio_detection_pipeline.ipynb`):**

* Из аудиопотока строится мел-спектрограмма.
* Свёрточная нейронная сеть (CNN) оценивает вероятность наличия титров.
* Дополнительно вычисляется косинусное сходство с шаблоном заставки.
* Итоговая аудио-вероятность рассчитывается как взвешенная сумма оценок CNN (70%) и косинусного сходства (30%). Вероятности сглаживаются скользящим средним.

**Комбинированная модель (ноутбук `notebooks/combined_results_pipeline.ipynb`):**

* Признаки видео- и аудио-моделей объединяются и подаются в алгоритм случайного леса (Random Forest), учитывающий веса классов.
* Получаем итоговые таймкоды начала и конца основного содержания.

## Результаты

По результатам сравнения трёх моделей (аудио, видео и их комбинации):

<img alt="Сравнение моделей 1.png" src="img\Сравнение моделей 1.png"/>

<img alt="Сравнение моделей 2.png" src="img\Сравнение моделей 2.png"/>

<img alt="Сравнение моделей 3.png" src="img\Сравнение моделей 3.png"/>

| Модель                 | MAE начала эпизода  | MAE конца эпизода   |
| ---------------------- | ------------------- | ------------------- |
| Аудио-модель           | 2.42 ± 0.99 сек     | 7.11 ± 1.65 сек     |
| Видео-модель           | 0.47 ± 0.50 сек     | 1.63 ± 1.60 сек     |
| Комбинированная модель | **0.16 ± 0.05 сек** | **0.74 ± 0.34 сек** |

* Аудио-модель показала наименьшую точность.
* Видео-модель превосходит аудио-модель по точности в 4–5 раз.
* Мультимодальная модель продемонстрировала наилучшие результаты, подтверждая эффективность объединения различных типов данных.

## Пример работы моделей для одной серии:
<img alt="Пример работы 1.png" src="img\Пример работы 1.png" width="1500"/>

### Общая динамика на всём эпизоде:

**1. Аудио-модель**

* Визуально проявляет самую большую степень шума и нестабильности (частые резкие скачки вероятности).
* Ярко выраженные пики вероятности наблюдаются по всему эпизоду, что указывает на частые ложные срабатывания из-за фоновой музыки или смены звуковых сцен.

**2. Видео-модель**

* Демонстрирует заметно более гладкую кривую вероятностей по сравнению с аудио-моделью.
* Отдельные выраженные пики связаны с визуальным оформлением сцен, напоминающим титры (например, титры в конце эпизода).
* В середине эпизода модель практически не имеет высоких всплесков, показывая стабильное различение обычных кадров и заставочных элементов.

**3. Комбинированная модель (аудио + видео)**

* Обеспечивает наиболее стабильные и точные результаты.
* Визуально более плавная и уверенная кривая вероятности, близкая к идеальному выделению основного контента и титров.
* Явно видно влияние обоих сигналов (аудио и видео), но влияние ложных аудио-срабатываний значительно снижено благодаря учёту видео-признаков.

<img alt="Пример работы 2.png" src="img\Пример работы 2.png"/>

### Анализ на старте эпизода (первые 60 секунд):

**Аудио-модель:**

* Вероятность титров постепенно растёт и выходит на стабильный уровень только к 10 секунде, с последующими колебаниями.
* Присутствуют значительные колебания между 0.7 и 1.0, показывающие неопределенность модели в распознавании именно звукового сопровождения титров.
* Таймкод начала (0:00:52) немного смещён от истинного (0:00:55), ошибка ≈3 секунды.

**Видео-модель:**

* Практически с самого начала показывает стабильно высокую вероятность близкую к 1.0, уверенно распознавая титры по визуальным признакам.
* Определяет начало содержания с минимальной погрешностью (0:00:54 против истинного 0:00:55).

**Комбинированная модель:**

* Плавная кривая вероятности, стабильно близкая к 1.0 на всем промежутке заставки.
* Наиболее точная в определении старта эпизода (0:00:55), идеально совпадающая с истинной меткой.
* Эффективно устраняет неопределённость аудио-модели и использует стабильность видео-модели для точного результата.

<img alt="Пример работы 3.png" src="img\Пример работы 3.png"/>

### Анализ на конце эпизода (последние 90 секунд):

**Аудио-модель:**

* До 2930 секунды модель неопределённа и вероятность титров низкая, затем резко возрастает к моменту появления финальных титров.
* Высокий разброс вероятности (колебания в пределах от 0.0 до 1.0), что подтверждает низкую стабильность.
* Таймкод окончания (0:48:58) имеет ошибку в ≈4 секунды относительно истинного (0:48:54).

**Видео-модель:**

* Модель быстро и чётко реагирует на появление визуальных титров, вероятность резко повышается к 2935 секунде и стабилизируется.
* Дает весьма точный таймкод конца (0:48:59), ошибка минимальна (≈5 секунд).

**Комбинированная модель:**

* Комбинированная модель уверенно находит момент появления титров, вероятность резко и стабильно возрастает точно в момент начала заставки.
* Выделяет финальные титры с максимальной точностью (таймкод 0:48:53), ошибка всего ≈1 секунда.
* Комбинация признаков позволяет сгладить небольшие ошибки видео и значительные ошибки аудио, достигая наилучшего результата.

### Зависимости и логика работы моделей:

* **Аудио-модель:**
  Основная проблема в низкой стабильности, ложных срабатываниях из-за фоновых звуков и неспецифических шумов. Лучше всего реагирует на музыкальные вставки заставок, но легко «ошибается» на других звуковых эффектах и переходах сцен.

* **Видео-модель:**
  Высокая стабильность в распознавании титров благодаря визуальным паттернам, очень точна при наличии явных визуальных признаков. Основная слабость – ложные срабатывания при наличии сцен с графическими элементами, напоминающими титры.

* **Комбинированная модель (аудио + видео):**
  Использует сильные стороны обеих моделей, компенсируя их слабости. Визуальные признаки обеспечивают стабильность, аудио – дополнительную уверенность в распознавании титров. Итоговая модель лучше всего справляется с неопределённостью и дает наименьшую ошибку.

### Итоговый вывод по анализу:

Комбинированная мультимодальная модель является наиболее подходящим решением для задачи выделения точных таймкодов начала и конца основного содержания эпизодов сериалов. Мультимодальный подход позволяет достичь наилучшей точности, стабильности и уверенности в результатах.

## Дальнейшие планы

* Добавить поддержку сериалов с предварительным содержанием (cold open).
* Расширение датасета другими типами контента (фильмы, ток-шоу).
* Внедрение мультиязычной поддержки и анализа субтитров.
* Оптимизация архитектуры для работы на мобильных устройствах и в режиме реального времени.
* Создание веб-интерфейса и API для удобной интеграции решения в платформы онлайн-кинотеатров.

[Ссылка на презентацию для защиты](https://docs.google.com/presentation/d/1p3sGUXsCtW5psnBDD3U0GqVvDEtSpaxlccS_rBnaipQ/edit?usp=sharing)

## Список литературы:
1. **Korolkov V., Yanchenko A.**
   *Automatic Detection of Intro and Credits in Video using CLIP and Multi-Head Attention* — arXiv:2504.09738, 2025. ([arxiv.org][1])

2. **Hao X. и др.**
   *Intro and Recap Detection for Movies and TV Series*. Proceedings of WACV 2021, pp. 13-22. ([openaccess.thecvf.com][2])

3. **Radford A. и др.**
   *Learning Transferable Visual Models from Natural Language Supervision (CLIP)*. ICML 2021. ([arxiv.org][3])

4. **Bertasius G., Wang H., Torresani L.**
   *Is Space-Time Attention All You Need for Video Understanding? (TimeSformer)*. arXiv:2102.05095, 2021. ([arxiv.org][4])

5. **Tong Z. и др.**
   *VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training*. arXiv:2203.12602, 2022. ([arxiv.org][5])

6. **Carreira J., Zisserman A.**
   *Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset* (I3D). CVPR 2017. ([arxiv.org][6])

7. **Park D. S. и др.**
   *SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition*. INTERSPEECH 2019. ([arxiv.org][7])

8. **Bertasius G., Wang H., Torresani L.**
   *Video Transformers: A Survey*. arXiv:2201.05991, 2022 — полезное обзорное исследование, обобщающее применения трансформеров к видео-задачам. ([arxiv.org][8])

Эти работы лежат в основе выбранной архитектуры (CLIP + Transformer для видео, CNN + SpecAugment для аудио), методов аугментации и подходов к мультимодальному объединению признаков, а также предоставляют сравнимые решения для задачи автоматического пропуска заставок и титров.

[1]: https://arxiv.org/html/2504.09738v1 "Automatic Detection of Intro and Credits in Video using CLIP and Multihead Attention"
[2]: https://openaccess.thecvf.com/content/WACV2021/papers/Hao_Intro_and_Recap_Detection_for_Movies_and_TV_Series_WACV_2021_paper.pdf "Intro and Recap Detection for Movies and TV Series"
[3]: https://arxiv.org/abs/2103.00020?utm_source=chatgpt.com "Learning Transferable Visual Models From Natural Language ..."
[4]: https://arxiv.org/abs/2102.05095?utm_source=chatgpt.com "Is Space-Time Attention All You Need for Video Understanding?"
[5]: https://arxiv.org/abs/2203.12602?utm_source=chatgpt.com "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training"
[6]: https://arxiv.org/abs/1705.07750?utm_source=chatgpt.com "Quo Vadis, Action Recognition? A New Model and the Kinetics ..."
[7]: https://arxiv.org/abs/1904.08779?utm_source=chatgpt.com "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition"
[8]: https://arxiv.org/pdf/2201.05991?utm_source=chatgpt.com "[PDF] Video Transformers: A Survey - arXiv"
