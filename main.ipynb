{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce9bbae0c7ae70e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Реализация метода CLIP + Multibead Attention\n",
    "\n",
    "Цель: Классифицировать каждую секунду видео как \"интро/титры\" (1) или \"основной контент\" (0)\n",
    "1. **Преобразуем видео в последовательность секунд**\n",
    "   Видео уже порезано до 1 кадра в секунду (1 FPS)—это оптимальный компромисс по точности и скорости .\n",
    "\n",
    "   > 1 кадр = 1 секунда времени. Длина эпизода в секундах = число кадров папки.\n",
    "\n",
    "2. **Формируем скользящие окна по 60 кадров**\n",
    "   *Окно* длиной ровно 1 мин. прокатываем по всей серии без пропусков:\n",
    "\n",
    "   ```\n",
    "   кадры 0-59, 1-60, 2-61, …  \n",
    "   ```\n",
    "\n",
    "   Такое «окно» даёт модели вид сразу на минуту истории и подходящее число параметров .\n",
    "\n",
    "3. **Извлекаем признаки CLIP для каждого кадра**\n",
    "   Каждый кадр 224×224 пропускаем через изображ-ный энкодер CLIP-ViT-B/32 и получаем вектор `f_t` размерности 512 .\n",
    "   Для 60 кадров окна складывается матрица `(60 × 512)`.\n",
    "\n",
    "4. **Добавляем обучаемые позиционные эмбеддинги**\n",
    "   Чтобы модель «знала», какой кадр раньше/позже, к каждому вектору `f_t` прибавляем свой маленький вектор `P_t` (учится вместе с моделью) .\n",
    "\n",
    "5. **Прогоняем окно через 16-слойный блок Multi-Head Attention**\n",
    "   16 слоёв × 16 голов—эта глубина показала максимум F1 без переобучения .\n",
    "   Attention позволяет «видеть» долгие зависимости: например, титры длятся десятки кадров.\n",
    "\n",
    "6. **Классифицируем каждый кадр независимо**\n",
    "   На выходе - 60 однонейронных «голов» (линейные слои) с сигмоидой, каждая говорит:\n",
    "\n",
    "   ```\n",
    "   ŷ_t  ≈ 1  -> кадр = интро/титры  \n",
    "   ŷ_t  ≈ 0  -> кадр = основное содержание\n",
    "   ```\n",
    "\n",
    "7. **Обучаемся по бинарной кросс-энтропии**\n",
    "   Потеря считается по каждому кадру окна .\n",
    "   *Гиперпараметры из статьи*: Adam, `lr = 5e-5`, `batch_size = 8`, seed `42`.\n",
    "\n",
    "8. **Получаем таймкоды**\n",
    "\n",
    "   * **Конец интро** = последняя подряд секунда, где `p(t) > thr_start` (порог 0 .6).\n",
    "   * **Начало титров** = первая секунда с `p(t) > thr_end` (порог 0 .55) ближе к концу серии.\n",
    "     Те же пороги использовались авторами (видно на графике)\n",
    "\n",
    "<img src=\"arcitecture.png\" alt=\"architecture\" width=\"1000\"/>\n",
    "\n",
    "9. **Метрики**\n",
    "   *Кадровые*: Accuracy, Precision, Recall, F1.\n",
    "   *Граничные*: абсолютная ошибка (сек) начала/конца.\n",
    "\n",
    "10. **Визуализация**\n",
    "    Строим график `p(t)` + горизонтальные пороги + вертикальные линии, как на вашем примере.\n",
    "\n",
    "### Импорт и базовые настройки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T09:12:40.413656Z",
     "start_time": "2025-06-18T09:12:36.279523Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import warnings, os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # глушим низкоуровневые логи TF\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning,\n",
    "                        module=\"transformers.image_processing_utils\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "num = np.random.randint(0, 1000)\n",
    "# Конфигурация\n",
    "DATA_ROOT = Path('.')\n",
    "FRAMES_DIR = DATA_ROOT / 'data' / 'frames'\n",
    "AUDIO_DIR = DATA_ROOT / 'data' / 'audio'\n",
    "CLIP_DIR = DATA_ROOT / 'data' / 'clip_features'  # Новая папка для CLIP-фичей\n",
    "LABELS_PATH = DATA_ROOT / 'data' / 'labels.csv'\n",
    "MODEL_SAVE_PATH = f\"models/clip_transformer_model_{num}.pth\"\n",
    "\n",
    "# Создаем папки\n",
    "CLIP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Гиперпараметры\n",
    "WINDOW_SIZE = 60\n",
    "BATCH_SIZE = 8\n",
    "LR = 5e-5\n",
    "EPOCHS = 10\n",
    "\n",
    "# Определение устройства\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Загрузка CLIP модель\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\"\n",
    ")\n",
    "clip_model = clip_model.to(device).eval()\n",
    "\n",
    "\n",
    "def time_to_seconds(time_str):\n",
    "    \"\"\"Конвертирует время в формате HH:MM:SS в секунды\"\"\"\n",
    "    h, m, s = time_str.split(':')\n",
    "    return int(h) * 3600 + int(m) * 60 + int(s)\n",
    "\n",
    "\n",
    "# Загрузка разметки\n",
    "labels_df = pd.read_csv(LABELS_PATH)\n",
    "\n",
    "# Преобразуем временные метки в секунды\n",
    "labels_df['start_sec'] = labels_df['start_main'].apply(time_to_seconds)\n",
    "labels_df['end_sec'] = labels_df['end_main'].apply(time_to_seconds)\n",
    "\n",
    "# Извлекаем название шоу и эпизода\n",
    "labels_df['show'] = labels_df['file'].apply(lambda x: x.split('/')[0])\n",
    "labels_df['episode_name'] = labels_df['file'].apply(\n",
    "    lambda x: os.path.splitext(os.path.basename(x))[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca33b8a4dc85f7f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Загрузка и предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d8f722861588b03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T09:12:48.316746Z",
     "start_time": "2025-06-18T09:12:47.420095Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка и извлечение отсутствующих CLIP-фичей...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking CLIP features: 100%|██████████| 19/19 [00:00<00:00, 43.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создание датасета...\n",
      "Статистика по кадрам:\n",
      "show1: 18640 кадров\n",
      "show2: 37141 кадров\n",
      "show1: 4 эпизодов в train, 2 в test\n",
      "show2: 10 эпизодов в train, 3 в test\n",
      "Размер train датасета: 40080\n",
      "Размер test датасета: 15701\n"
     ]
    }
   ],
   "source": [
    "# Функция для проверки полноты CLIP-фичей\n",
    "def check_clip_features_complete(show, episode_name):\n",
    "    \"\"\"Проверяет, все ли CLIP-фичи для эпизода уже извлечены\"\"\"\n",
    "    frames_dir = FRAMES_DIR / show / episode_name\n",
    "    clip_dir = CLIP_DIR / show / episode_name\n",
    "\n",
    "    if not frames_dir.exists() or not clip_dir.exists():\n",
    "        return False\n",
    "\n",
    "    frame_files = set(f.stem for f in frames_dir.glob('*.jpg'))\n",
    "    clip_files = set(f.stem for f in clip_dir.glob('*.npy'))\n",
    "\n",
    "    return frame_files == clip_files\n",
    "\n",
    "\n",
    "# Функция для извлечения отсутствующих CLIP-фичей\n",
    "def extract_missing_clip_features():\n",
    "    \"\"\"Извлекает только отсутствующие CLIP-фичи\"\"\"\n",
    "    for idx, row in tqdm(labels_df.iterrows(), total=len(labels_df), desc=\"Checking CLIP features\"):\n",
    "        show = row['show']\n",
    "        episode_name = row['episode_name']\n",
    "        frames_dir = FRAMES_DIR / show / episode_name\n",
    "\n",
    "        if not frames_dir.exists():\n",
    "            print(f\"Папка с фреймами не найдена: {frames_dir}\")\n",
    "            continue\n",
    "\n",
    "        # Проверяем, все ли фичи уже извлечены\n",
    "        if check_clip_features_complete(show, episode_name):\n",
    "            continue\n",
    "\n",
    "        # Папка для сохранения CLIP-фичей\n",
    "        clip_episode_dir = CLIP_DIR / show / episode_name\n",
    "        clip_episode_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Обрабатываем каждый кадр\n",
    "        for frame_path in sorted(frames_dir.glob('*.jpg'), key=lambda x: int(x.stem)):\n",
    "            clip_path = clip_episode_dir / f\"{frame_path.stem}.npy\"\n",
    "\n",
    "            # Пропускаем уже существующие фичи\n",
    "            if clip_path.exists():\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                img = Image.open(frame_path).convert('RGB')\n",
    "                inputs = clip_processor(images=img, return_tensors=\"pt\").to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = clip_model.get_image_features(**inputs)\n",
    "\n",
    "                # Сохраняем фичи\n",
    "                features = outputs.cpu().numpy()\n",
    "                np.save(clip_path, features)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка обработки {frame_path}: {e}\")\n",
    "                # Создаем нулевой вектор в случае ошибки\n",
    "                np.save(clip_path, np.zeros((1, 512)))\n",
    "\n",
    "\n",
    "# Вызываем функцию для извлечения отсутствующих фичей\n",
    "print(\"Проверка и извлечение отсутствующих CLIP-фичей...\")\n",
    "extract_missing_clip_features()\n",
    "\n",
    "\n",
    "class ClipFeaturesDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Датасет для загрузки предварительно вычисленных CLIP-фичей\"\"\"\n",
    "\n",
    "    def __init__(self, labels_df):\n",
    "        self.samples = []  # (путь к фичам, метка, show, episode_name)\n",
    "\n",
    "        for idx, row in labels_df.iterrows():\n",
    "            show = row['show']\n",
    "            episode_name = row['episode_name']\n",
    "            start_sec = row['start_sec']\n",
    "            end_sec = row['end_sec']\n",
    "\n",
    "            clip_dir = CLIP_DIR / show / episode_name\n",
    "            if not clip_dir.exists():\n",
    "                print(f\"Папка CLIP-фичей не найдена: {clip_dir}\")\n",
    "                continue\n",
    "\n",
    "            # Получаем список фичей\n",
    "            clip_files = sorted(clip_dir.glob('*.npy'), key=lambda x: int(x.stem))\n",
    "\n",
    "            # Создаем семплы\n",
    "            for i, clip_path in enumerate(clip_files):\n",
    "                time_sec = i\n",
    "                label = 1 if (time_sec < start_sec or time_sec > end_sec) else 0\n",
    "                self.samples.append((clip_path, label, show, episode_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        clip_path, label, show, episode_name = self.samples[idx]\n",
    "        features = np.load(clip_path)\n",
    "        return torch.tensor(features).squeeze(0), label\n",
    "\n",
    "\n",
    "# Создаем датасет\n",
    "print(\"Создание датасета...\")\n",
    "full_dataset = ClipFeaturesDataset(labels_df)\n",
    "shows = labels_df['show'].unique()\n",
    "\n",
    "# Собираем статистику по кадрам\n",
    "show_frame_counts = {}\n",
    "for sample in full_dataset.samples:\n",
    "    _, _, show, _ = sample\n",
    "    show_frame_counts[show] = show_frame_counts.get(show, 0) + 1\n",
    "\n",
    "print(\"Статистика по кадрам:\")\n",
    "for show, count in show_frame_counts.items():\n",
    "    print(f\"{show}: {count} кадров\")\n",
    "\n",
    "# Разделение эпизодов внутри каждого шоу\n",
    "train_samples = []\n",
    "test_samples = []\n",
    "\n",
    "for show in shows:\n",
    "    # Получаем все эпизоды этого шоу\n",
    "    episodes = set()\n",
    "    for sample in full_dataset.samples:\n",
    "        _, _, s, ep = sample\n",
    "        if s == show:\n",
    "            episodes.add(ep)\n",
    "\n",
    "    episodes = sorted(episodes)\n",
    "    split_idx = int(0.8 * len(episodes))\n",
    "    train_episodes = episodes[:split_idx]\n",
    "    test_episodes = episodes[split_idx:]\n",
    "\n",
    "    print(f\"{show}: {len(train_episodes)} эпизодов в train, {len(test_episodes)} в test\")\n",
    "\n",
    "    # Распределяем сэмплы\n",
    "    for sample in full_dataset.samples:\n",
    "        _, _, s, ep = sample\n",
    "        if s == show:\n",
    "            if ep in train_episodes:\n",
    "                train_samples.append(sample)\n",
    "            elif ep in test_episodes:\n",
    "                test_samples.append(sample)\n",
    "\n",
    "\n",
    "# Создаем новые датасеты\n",
    "class FilteredDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        clip_path, label, _, _ = self.samples[idx]\n",
    "        features = np.load(clip_path)\n",
    "        return torch.tensor(features).squeeze(0), label\n",
    "\n",
    "\n",
    "train_data = FilteredDataset(train_samples)\n",
    "test_data = FilteredDataset(test_samples)\n",
    "\n",
    "print(f\"Размер train датасета: {len(train_data)}\")\n",
    "print(f\"Размер test датасета: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0a727631d31c78",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Подготовка последовательностей (скользящие окна)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af1536e298ca1d54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T09:15:11.660300Z",
     "start_time": "2025-06-18T09:12:54.401023Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_sequences(features, labels, window_size=60):\n",
    "    sequences = []\n",
    "    seq_labels = []\n",
    "\n",
    "    for i in range(len(features) - window_size + 1):\n",
    "        seq = features[i:i + window_size]\n",
    "        lbl = labels[i:i + window_size]\n",
    "        sequences.append(seq)\n",
    "        seq_labels.append(lbl)\n",
    "\n",
    "    return np.array(sequences), np.array(seq_labels)\n",
    "\n",
    "\n",
    "# Собираем фичи и метки для train\n",
    "train_features = []\n",
    "train_labels = []\n",
    "for features, label in train_data:\n",
    "    train_features.append(features.numpy())\n",
    "    train_labels.append(label)\n",
    "\n",
    "train_sequences, train_seq_labels = create_sequences(train_features, train_labels, WINDOW_SIZE)\n",
    "\n",
    "# Собираем фичи и метки для test\n",
    "test_features = []\n",
    "test_labels = []\n",
    "for features, label in test_data:\n",
    "    test_features.append(features.numpy())\n",
    "    test_labels.append(label)\n",
    "\n",
    "test_sequences, test_seq_labels = create_sequences(test_features, test_labels, WINDOW_SIZE)\n",
    "\n",
    "# Конвертация в тензоры PyTorch\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(train_sequences).float(),\n",
    "    torch.tensor(train_seq_labels).float()\n",
    ")\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(test_sequences).float(),\n",
    "    torch.tensor(test_seq_labels).float()\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a06619564e5853",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Модель Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a641f6d0202d56e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T09:16:19.405906Z",
     "start_time": "2025-06-18T09:16:19.246868Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=60):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_embedding[:, :x.size(1), :]\n",
    "\n",
    "\n",
    "class VideoTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=512, num_layers=16, nhead=16):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncoding(input_dim)\n",
    "\n",
    "        # Трансформер (16 слоев, 16 голов)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_dim,\n",
    "            nhead=nhead,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # 60 независимых классификаторов\n",
    "        self.classifiers = nn.ModuleList([\n",
    "            nn.Linear(input_dim, 1) for _ in range(WINDOW_SIZE)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        outputs = []\n",
    "        for i in range(WINDOW_SIZE):\n",
    "            outputs.append(self.classifiers[i](x[:, i]))\n",
    "\n",
    "        return torch.stack(outputs, dim=1).squeeze(-1)\n",
    "\n",
    "\n",
    "model = VideoTransformer().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad351bf2620feae9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95fdd4fdfaa43ab9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T10:56:18.365235Z",
     "start_time": "2025-06-18T09:16:21.521768Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5003/5003 [2:40:00<00:00,  1.92s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 0.1306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5003/5003 [2:27:50<00:00,  1.77s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Loss: 0.1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5003/5003 [2:18:50<00:00,  1.67s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Loss: 0.1398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5003/5003 [2:21:43<00:00,  1.70s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Loss: 0.1398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5003/5003 [2:40:08<00:00,  1.92s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Loss: 0.1401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5003/5003 [2:37:01<00:00,  1.88s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Loss: 0.1399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5003/5003 [2:34:42<00:00,  1.86s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Loss: 0.1397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1007/5003 [31:38<2:11:55,  1.98s/it]"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Temporal Shifting (регуляризация из статьи)\n",
    "        if np.random.rand() > 0.5:\n",
    "            shift = np.random.randint(-5, 6)\n",
    "            inputs = torch.roll(inputs, shifts=shift, dims=1)\n",
    "            labels = torch.roll(labels, shifts=shift, dims=1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loss = train_epoch(model, train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} | Loss: {loss:.4f}\")\n",
    "\n",
    "# Сохранение модели\n",
    "torch.save(model.state_dict(), MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d45b00a9b018a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Оценка и метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2cac70c278bde4e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(loader, desc=\"Evaluating\"):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy() > 0.5\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels.numpy())\n",
    "\n",
    "    preds = np.vstack(all_preds).flatten()\n",
    "    labels = np.vstack(all_labels).flatten()\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='binary', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return acc, precision, recall, f1\n",
    "\n",
    "\n",
    "acc, precision, recall, f1 = evaluate(model, test_loader)\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f75bbf1d1561caa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f00fbb8487a2ce",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Визуализация предсказаний "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e6eda0c7a525a74",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_predictions(episode_features, true_labels):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = torch.tensor(episode_features).unsqueeze(0).to(device)\n",
    "        outputs = model(inputs)\n",
    "        probs = torch.sigmoid(outputs).cpu().numpy().flatten()\n",
    "\n",
    "    # Постобработка: медианный фильтр\n",
    "    probs_smoothed = np.convolve(probs, np.ones(5) / 5, mode='same')\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(probs_smoothed, 'b-', label='Predicted Probability')\n",
    "    plt.axhline(y=0.5, color='r', linestyle='--', label='Decision Threshold')\n",
    "    plt.plot(true_labels, 'g-', alpha=0.5, label='True Labels')\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('Intro/Credits Detection')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('detection_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Пример визуализации для первого тестового эпизода\n",
    "plot_predictions(\n",
    "    test_sequences[0],\n",
    "    test_seq_labels[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1718f3e514f4bff0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Определение таймкодов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "162b81a4df2ad9ee",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def detect_timestamps(probs, threshold=0.5):\n",
    "    \"\"\"Определяет таймкоды начала и конца основного контента\"\"\"\n",
    "    # Начало интро: первый кадр с prob > threshold\n",
    "    start_intro = np.argmax(probs > threshold)\n",
    "\n",
    "    # Конец интро: последний кадр перед основным контентом\n",
    "    end_intro = start_intro\n",
    "    while end_intro < len(probs) and probs[end_intro] > threshold:\n",
    "        end_intro += 1\n",
    "\n",
    "    # Начало титров: первый кадр титров в конце\n",
    "    start_credits = len(probs) - np.argmax(probs[::-1] > threshold) - 1\n",
    "\n",
    "    return {\n",
    "        'start_main': end_intro + 1,  # начало контента после интро\n",
    "        'end_main': start_credits  # конец контента перед титрами\n",
    "    }\n",
    "\n",
    "\n",
    "# Пример использования для первого тестового эпизода\n",
    "with torch.no_grad():\n",
    "    inputs = torch.tensor(test_sequences[0]).unsqueeze(0).to(device)\n",
    "    outputs = model(inputs)\n",
    "    probs = torch.sigmoid(outputs).cpu().numpy().flatten()\n",
    "\n",
    "timestamps = detect_timestamps(probs)\n",
    "print(f\"Start Main: {timestamps['start_main']} sec\")\n",
    "print(f\"End Main: {timestamps['end_main']} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c749f405e60169bc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
